# -*- coding: utf-8 -*-
"""XRD_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yA3bNPzwTxPZbi-Ju_e-RkQbLGXAdx3P
"""

import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
from sklearn.kernel_ridge import KernelRidge
#from google.colab import files
import tensorflow as tf
import math
import pandas

"""#Import data"""

from google.colab import drive #Подключение диска с файлами
drive.mount('/content/drive')
#dataframe = pandas.read_csv('drive/My Drive/Colab Notebooks/XRD.dat', usecols=[1], engine='python')
dataframe = pandas.read_csv('drive/My Drive/Colab Notebooks/XRD.dat',sep='\t')
dataset = dataframe.values
train_size = int(len(dataset))
dataset.shape
x_data = dataset[0:train_size, 0]
y_data = dataset[0:train_size, 1]

"""# Display the dataset"""

plt.figure(0)
plt.scatter(x_data[::1], y_data[::1], s=2)
plt.grid()
plt.show()
plt.savefig('dataset.png',dpi=300)
#files.download('dataset.png')

"""#Create and train Kernel Ridge Regression model"""

krr=KernelRidge(alpha=5.0, gamma=0.05, kernel='rbf')
list_x = np.array(x_data).reshape(-1, 1)
list_y = np.array(y_data).reshape(-1, 1)
krr.fit( list_x, list_y)
KernelRidge(alpha=5.0, gamma=0.05, kernel='rbf')
y_predicted_krr=krr.predict(list_x)
plt.figure(1)
plt.scatter(x_data[::1], y_data[::1], s=1)
plt.plot(x_data, y_predicted_krr, 'r', linewidth=2)
plt.grid()

"""# Create NN model"""

model = keras.Sequential()
model.add(keras.layers.Dense(units = 1, activation = 'linear', input_shape=[1]))
model.add(keras.layers.Dense(units = 128, activation = 'softplus'))
model.add(keras.layers.Dense(units = 128, activation = 'softplus'))
model.add(keras.layers.Dense(units = 128, activation = 'softplus'))
model.add(keras.layers.Dense(units = 128, activation = 'softplus'))
model.add(keras.layers.Dense(units = 1, activation = 'linear'))
model.compile(loss='mse', optimizer="adam")

"""# Display the model"""

model.summary()

"""# Training NN model"""

history=model.fit( x_data, y_data, epochs=300,batch_size=50, verbose=2)
plt.plot(history.history['loss'], label='train')

"""# Compute and display the output"""

y_predicted = model.predict(x_data)
plt.figure(1)
plt.scatter(x_data[::1], y_data[::1], s=1)
plt.plot(x_data, y_predicted, 'r', linewidth=2)
plt.grid()

y_p=y_predicted.reshape(-1)
xid = np.linspace(300, 1800, num=100)
plt.scatter(y_data[::1], y_p[::1], s=1)
plt.plot(xid, xid, 'r', linewidth=2)

r = np.corrcoef(y_data, y_p)
r[0,1]

err=y_p-y_data
plt.scatter(x_data[::1], err[::1], s=1)

import seaborn as sns
sns_plot = sns.distplot(err)
fig = sns_plot.get_figure()



import scipy
mean=np.mean(err)
kurt=scipy.stats.kurtosis(err)
skw=scipy.stats.skew(err)
print('mean==%.4f, skew=%.4f, kurtosis=%.4f' % (mean,skw, kurt))

from scipy.stats import shapiro
stat, p = shapiro(err)
print('stat=%.4f, p=%.4f' % (stat, p))
if p > 0.05:
 print('Probably Gaussian')
else:
 print('Probably not Gaussian')

# Example of the D'Agostino's K^2 Normality Test
from scipy.stats import normaltest
stat, p = normaltest(err)
print('stat=%.3f, p=%.3f' % (stat, p))
if p > 0.05:
 print('Probably Gaussian')
else:
 print('Probably not Gaussian')